{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Management is considering using a language model to classify written customer reviews and call and complaint logs. The goal is to improve customer service by identifying critical customer messages to assign to support staff. Our project is to develop a sentiment identifying system using RNNs. Movie reviews are used to train the system in order to select the best method of training the model to identify positive and negative language. Predefined embeddings are used for the training. The following embeddings were used in the study glove.6B.50d and glove.6B.100d. Different types of RNN models were used in the study. The BasicRNNCell using tensorflow was first experimented with but the best results obtained were a train accuracy of .83 and a test accuracy of .66 with 50 epochs, one RNN layer and an adam optimizer. So the study progressed to using a BasicLSTMCell under Keras the results and methods are discussed in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above libraries are loaded and constants are initialized as well as the reset graph method is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "#Load embeddings glove.6B.50d.txt and glove.6B.100d.txt\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "glove_6B_50dfilename = 'glove.6B.50d.txt'\n",
    "embeddings1_filename = os.path.join(embeddings_directory, glove_6B_50dfilename)\n",
    "\n",
    "glove_6B_100dfilename = 'glove.6B.100d.txt'\n",
    "embeddings2_filename = os.path.join(embeddings_directory, glove_6B_100dfilename)\n",
    "\n",
    "word1_to_index, index_to_embedding1 = \\\n",
    "    load_embedding_from_disks(embeddings1_filename, with_indexes=True)\n",
    "word2_to_index, index_to_embedding2 = \\\n",
    "    load_embedding_from_disks(embeddings2_filename, with_indexes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell defines the load_embedding_from_disks method and loads the two embeddings used in this study glove.6B.50d and glove.6B.100d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_dim_tests(word_to_index, index_to_embedding):\n",
    "\n",
    "    vocab_size, embedding_dim = index_to_embedding.shape\n",
    "    word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "    idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "    complete_vocabulary_size = idx \n",
    "    embd = list(np.array(index_to_embedding[idx], dtype=int)) \n",
    "    word = \"the\"\n",
    "    idx = word_to_index[word]\n",
    "    embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "\n",
    "    # Show how to use embeddings dictionaries with a test sentence\n",
    "    # This is a famous typing exercise with all letters of the alphabet\n",
    "    # https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "    a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "    words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "    for word in words_in_test_sentence:\n",
    "        word_ = word.lower()\n",
    "        embedding = index_to_embedding[word_to_index[word_]]\n",
    "    return words_in_test_sentence, embedding, word_to_index, \\\n",
    "           index_to_embedding, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Helper function to define vocabulary size\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding# \n",
    "\n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "def define_vocabulary_size(index_to_embedding):\n",
    "    limited_word_to_index = defaultdict(default_factory, \\\n",
    "        {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "    # Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "    limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "    # Set the unknown-word row to be all zeros as previously\n",
    "    limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "        index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "            reshape(1,embedding_dim), \n",
    "        axis = 0)\n",
    "\n",
    "    # Delete large numpy array to clear some CPU RAM\n",
    "    del index_to_embedding\n",
    "\n",
    "    # Verify the new vocabulary: should get same embeddings for test sentence\n",
    "    # Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "    for word in words_in_test_sentence:\n",
    "        word_ = word.lower()\n",
    "        embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    return embedding, limited_index_to_embedding, limited_word_to_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two cells are used to prepare the embeddings to convert the negative and positive reviews for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove\n",
    "\n",
    "    # text parsing function for creating text documents \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are helper methods used to retrieve then convert negative and positive reviews to documents. One of the methods is used to retrieve the files. The other method is used to parse the files including flagging words not to be added to the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "for i in range(num_files):\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    negative_documents.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "for i in range(num_files):\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    positive_documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two cells retrieve negative and positive reviews. All the files in the negative and positive folders are read. Each word in the file is tokenized then appended to it's respective neg pos document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# -----------------------------------------------------\n",
    "def cvt_pos_neg_npa(embedding, limited_index_to_embedding, \n",
    "                    index_to_embedding, limited_word_to_index):\n",
    "    max_review_length = 0  # initialize\n",
    "    for doc in negative_documents:\n",
    "        max_review_length = max(max_review_length, len(doc))    \n",
    "    for doc in positive_documents:\n",
    "        max_review_length = max(max_review_length, len(doc)) \n",
    "\n",
    "    min_review_length = max_review_length  # initialize\n",
    "    for doc in negative_documents:\n",
    "        min_review_length = min(min_review_length, len(doc))    \n",
    "    for doc in positive_documents:\n",
    "        min_review_length = min(min_review_length, len(doc)) \n",
    "\n",
    "    # construct list of 1000 lists with 40 words in each list\n",
    "    from itertools import chain\n",
    "    documents = []\n",
    "    for doc in negative_documents:\n",
    "        doc_begin = doc[0:20]\n",
    "        doc_end = doc[len(doc) - 20: len(doc)]\n",
    "        documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "    for doc in positive_documents:\n",
    "        doc_begin = doc[0:20]\n",
    "        doc_end = doc[len(doc) - 20: len(doc)]\n",
    "        documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "    # create list of lists of lists for embeddings\n",
    "    embeddings = []    \n",
    "    for doc in documents:\n",
    "        embedding = []\n",
    "        for word in doc:\n",
    "            embedding.append(limited_index_to_embedding[limited_word_to_index\\\n",
    "                                                        [word]]) \n",
    "        embeddings.append(embedding)\n",
    "    return embeddings, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell converts the negative positive results to numpy arrays and an embeddings list is created for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "def create_train_test(embeddings):\n",
    "    embeddings_array = np.array(embeddings)\n",
    "\n",
    "    # Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "    thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                          np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "    # Scikit Learn for random splitting of the data  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Random splitting of the data in to training (80%) and test (20%)  \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                         random_state = RANDOM_SEED)\n",
    "    return X_train, X_test, y_train, y_test, embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "The embedding list is split into train and test data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 823us/sample - loss: 0.6665 - acc: 0.6712\n",
      "200/200 [==============================] - 0s 928us/sample - loss: 0.6787 - acc: 0.6150\n",
      "800/800 [==============================] - 1s 930us/sample - loss: 0.5487 - acc: 0.6725\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.6486 - acc: 0.6150\n",
      "800/800 [==============================] - 1s 839us/sample - loss: 0.6928 - acc: 0.4950\n",
      "200/200 [==============================] - 0s 952us/sample - loss: 0.6922 - acc: 0.5150\n",
      "800/800 [==============================] - 1s 984us/sample - loss: 0.4501 - acc: 0.8075\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.6128 - acc: 0.6500\n",
      "800/800 [==============================] - 1s 866us/sample - loss: 0.5000 - acc: 0.7950\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.5949 - acc: 0.6850\n",
      "800/800 [==============================] - 1s 931us/sample - loss: 0.6939 - acc: 0.4950\n",
      "200/200 [==============================] - 0s 962us/sample - loss: 0.6926 - acc: 0.5200\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "EmbeddingSet = []\n",
    "Optimizer = []\n",
    "Training_Set_Accuracy = []\n",
    "Test_Set_Accuracy = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "rmsprop = tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6)\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6)\n",
    "\n",
    "for i in range(0,2):\n",
    "    if(i==0):\n",
    "        words_in_test_sentence, embedding, \\\n",
    "          word_to_index, index_to_embedding, embedding_dim = \\\n",
    "          define_dim_tests(word1_to_index, index_to_embedding1)\n",
    "        es='glove.6B.50d.txt'\n",
    "    else:\n",
    "        words_in_test_sentence, embedding, \\\n",
    "          word_to_index, index_to_embedding, embedding_dim = \\\n",
    "          define_dim_tests(word2_to_index, index_to_embedding2)\n",
    "        es='glove.6B.100d.txt'\n",
    "\n",
    "    embedding, limited_index_to_embedding, limited_word_to_index = \\\n",
    "      define_vocabulary_size(index_to_embedding)\n",
    "    embeddings, documents = cvt_pos_neg_npa(embedding, \n",
    "                                        limited_index_to_embedding, \n",
    "                                        index_to_embedding, \n",
    "                                        limited_word_to_index)   \n",
    "    X_train, X_test, y_train, y_test, embeddings_array = \\\n",
    "    create_train_test(embeddings)\n",
    "    n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "    n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "\n",
    "    for opt in [adam, rmsprop, sgd]:\n",
    "        #Referenced \n",
    "        #https://pythonprogramming.net/recurrent-neural-network-deep-learning-python-tensorflow-keras/\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, \n",
    "            input_shape=(X_train.shape[1:]), activation='relu', \n",
    "                       return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(128, \n",
    "            input_shape=(X_train.shape[1:]), activation='relu', \n",
    "                       return_sequences=True))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(LSTM(128, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               optimizer=opt,\n",
    "               metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "        model.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=10,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  verbose=0)\n",
    "\n",
    "        score, acc_train = model.evaluate(X_train, y_train)\n",
    "        score, acc_test = model.evaluate(X_test, y_test)\n",
    "        Training_Set_Accuracy.append(acc_train)\n",
    "        Test_Set_Accuracy.append(acc_test)\n",
    "        EmbeddingSet.append(es)\n",
    "        if(opt==adam):\n",
    "            Optimizer.append('adam')\n",
    "        elif(opt==rmsprop):\n",
    "            Optimizer.append('rmsprop')\n",
    "        else:\n",
    "            Optimizer.append('sgd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code cell configures, trains and evaluates the BasicLSTMCell model. The model contains a three-layer LSTM with two dense layers for training. There are two for loops to train the model in a 2 x 3 matrix of embeddings and optimizers. The first for loop is used to select the embedding to use as training data. The training and testing sets are built in the outer loop. Within the inner loop an optimizer is selected from the following list adam, rmsprop and sgd for the model. In the inner loop the model is configured, trained and evaluated. Statistical data is compiled here too. It is important to note that the training runs only required 10 epochs to reach levels that the BasicRNNCell couldn't reach using 50 epochs. For the learning rate .001 was used with a decay rate of 1e-6 so the steps would get smaller as gradiaent decent approached the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+-------------------+\n",
      "|     Embedding     | Optimizer | Train_Set_Accuracy | Test_Set_Accuracy |\n",
      "+-------------------+-----------+--------------------+-------------------+\n",
      "|  glove.6B.50d.txt |    adam   |       0.786        |        0.68       |\n",
      "|  glove.6B.50d.txt |  rmsprop  |       0.744        |       0.675       |\n",
      "|  glove.6B.50d.txt |    sgd    |       0.558        |        0.54       |\n",
      "| glove.6B.100d.txt |    adam   |       0.924        |       0.715       |\n",
      "| glove.6B.100d.txt |  rmsprop  |       0.842        |        0.68       |\n",
      "| glove.6B.100d.txt |    sgd    |       0.496        |       0.475       |\n",
      "+-------------------+-----------+--------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable(['Embedding', 'Optimizer', \n",
    "                     'Train_Set_Accuracy', 'Test_Set_Accuracy'])\n",
    "for x in range(0, 6):\n",
    "    table.add_row([EmbeddingSet[x], Optimizer[x], \n",
    "                   round(Training_Set_Accuracy[x], 3), \n",
    "                   round(Test_Set_Accuracy[x], 3)])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+-------------------+\n",
      "|     Embedding     | Optimizer | Train_Set_Accuracy | Test_Set_Accuracy |\n",
      "+-------------------+-----------+--------------------+-------------------+\n",
      "|  glove.6B.50d.txt |    adam   |       0.671        |       0.615       |\n",
      "|  glove.6B.50d.txt |  rmsprop  |       0.672        |       0.615       |\n",
      "|  glove.6B.50d.txt |    sgd    |       0.495        |       0.515       |\n",
      "| glove.6B.100d.txt |    adam   |       0.808        |        0.65       |\n",
      "| glove.6B.100d.txt |  rmsprop  |       0.795        |       0.685       |\n",
      "| glove.6B.100d.txt |    sgd    |       0.495        |        0.52       |\n",
      "+-------------------+-----------+--------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable(['Embedding', 'Optimizer', \n",
    "                     'Train_Set_Accuracy', 'Test_Set_Accuracy'])\n",
    "for x in range(0, 6):\n",
    "    table.add_row([EmbeddingSet[x], Optimizer[x], \n",
    "                   round(Training_Set_Accuracy[x], 3), \n",
    "                   round(Test_Set_Accuracy[x], 3)])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results of the study. There are two tables displayed above because data accuracy was inconsistent for each run during test and development. The first table represents the most accurate run and the second table is the last run. The two results agree the best combination is an embedding using glove.6B.100d and optimizer of adam. This combination produced the best results of train accuracy .924 and test accuracy .715. Senior management should choose per this study to use an RNN model with three BasicLSTMCell layers and two dense layers. The learning rate should be set to .001 along with a decay rate 1e-6, epochs 10 and neurons 20. To improve on the model, use a computer with Nvidia GPUs.  When constructing the model use a CudaLSTMCell instead of BasicLSTMCell. This would improve performance and allow the model to learn faster. After the model is trained using the suggested CudaLSTM model a pipeline should be created to route logged reviews entered to the model for prediction. If the customer review and call and compliant log is predicted to be a negative log a customer service rep should be notified. The notification triggers action to taken by the customer service rep. The model should be retrained on a regular basis or as prediction accuracy falls below a minimum threshold. The incorrect predictions should be included in the training data to improve predictions. It is important the customer service model be updated regularly to prevent the model from becoming out of date.  Word choice in vogue changes over time and new terms are coined which may have negative connotations. The new terms will be unfamiliar to the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
