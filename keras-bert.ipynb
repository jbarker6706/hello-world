{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "Requirement already satisfied: six in c:\\users\\jbark\\anaconda3\\lib\\site-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "max_seq_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First, we load the sample data IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - ETA:  - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 56s - ETA: 53 - ETA: 49 - ETA: 46 - ETA: 44 - ETA: 42 - ETA: 39 - ETA: 37 - ETA: 36 - ETA: 35 - ETA: 32 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 25 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 8s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very enjoyable film that features characters...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I sat through this turkey because I hadn't see...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massacre is a film directed by Andrea Bianchi ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I watched this knowing almost nothing about it...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I picked this up in the 'Danger After Dark' bo...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  polarity\n",
       "0  A very enjoyable film that features characters...        10         1\n",
       "1  I sat through this turkey because I hadn't see...         1         0\n",
       "2  Massacre is a film directed by Andrea Bianchi ...         4         0\n",
       "3  I watched this knowing almost nothing about it...         3         0\n",
       "4  I picked this up in the 'Danger After Dark' bo...         7         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "\n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "  return train_df, test_df\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "train_df, test_df = download_and_load_datasets()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets (Only take up to max_seq_length words for memory)\n",
    "train_text = train_df['sentence'].tolist()\n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = train_df['polarity'].tolist()\n",
    "\n",
    "test_text = test_df['sentence'].tolist()\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = test_df['polarity'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Next, tokenize our text to create `input_ids`, `input_masks`, and `segment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233087fb860e4078826c1ba5490b0c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f878335d6484049a87b24627d92bf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25000, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_label)\n",
    "test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 22,051,329\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6208/25000 [======>.......................] - ETA: 6:21:59 - loss: 0.6897 - acc: 0.46 - ETA: 5:45:33 - loss: 0.9544 - acc: 0.54 - ETA: 5:27:39 - loss: 0.9889 - acc: 0.52 - ETA: 5:40:39 - loss: 0.9553 - acc: 0.51 - ETA: 5:38:40 - loss: 0.9024 - acc: 0.51 - ETA: 5:25:04 - loss: 0.8826 - acc: 0.51 - ETA: 5:26:53 - loss: 0.8552 - acc: 0.51 - ETA: 5:30:28 - loss: 0.8629 - acc: 0.49 - ETA: 5:26:30 - loss: 0.8559 - acc: 0.49 - ETA: 5:19:44 - loss: 0.8429 - acc: 0.49 - ETA: 5:23:57 - loss: 0.8294 - acc: 0.48 - ETA: 5:22:14 - loss: 0.8187 - acc: 0.48 - ETA: 5:20:30 - loss: 0.8085 - acc: 0.49 - ETA: 5:17:47 - loss: 0.8013 - acc: 0.49 - ETA: 5:16:02 - loss: 0.7957 - acc: 0.49 - ETA: 5:15:36 - loss: 0.7894 - acc: 0.48 - ETA: 5:14:49 - loss: 0.7884 - acc: 0.48 - ETA: 5:15:33 - loss: 0.7919 - acc: 0.47 - ETA: 5:13:42 - loss: 0.7893 - acc: 0.47 - ETA: 5:11:40 - loss: 0.7887 - acc: 0.47 - ETA: 5:11:19 - loss: 0.7837 - acc: 0.48 - ETA: 5:10:47 - loss: 0.7800 - acc: 0.48 - ETA: 5:08:49 - loss: 0.7751 - acc: 0.49 - ETA: 5:09:29 - loss: 0.7759 - acc: 0.49 - ETA: 5:09:23 - loss: 0.7781 - acc: 0.49 - ETA: 5:10:50 - loss: 0.7750 - acc: 0.49 - ETA: 5:10:21 - loss: 0.7721 - acc: 0.49 - ETA: 5:12:27 - loss: 0.7676 - acc: 0.49 - ETA: 5:11:16 - loss: 0.7728 - acc: 0.49 - ETA: 5:10:08 - loss: 0.7732 - acc: 0.49 - ETA: 5:08:48 - loss: 0.7723 - acc: 0.48 - ETA: 5:08:29 - loss: 0.7697 - acc: 0.49 - ETA: 5:07:20 - loss: 0.7680 - acc: 0.49 - ETA: 5:06:37 - loss: 0.7653 - acc: 0.49 - ETA: 5:05:27 - loss: 0.7652 - acc: 0.49 - ETA: 5:05:29 - loss: 0.7630 - acc: 0.49 - ETA: 5:05:42 - loss: 0.7606 - acc: 0.49 - ETA: 5:05:27 - loss: 0.7622 - acc: 0.49 - ETA: 5:04:56 - loss: 0.7606 - acc: 0.49 - ETA: 5:04:30 - loss: 0.7607 - acc: 0.49 - ETA: 5:03:33 - loss: 0.7597 - acc: 0.50 - ETA: 5:02:52 - loss: 0.7602 - acc: 0.49 - ETA: 5:02:38 - loss: 0.7587 - acc: 0.49 - ETA: 5:01:53 - loss: 0.7596 - acc: 0.49 - ETA: 5:01:29 - loss: 0.7582 - acc: 0.49 - ETA: 5:01:41 - loss: 0.7564 - acc: 0.49 - ETA: 5:00:52 - loss: 0.7561 - acc: 0.49 - ETA: 5:01:04 - loss: 0.7553 - acc: 0.49 - ETA: 5:01:11 - loss: 0.7541 - acc: 0.49 - ETA: 5:01:15 - loss: 0.7530 - acc: 0.49 - ETA: 5:00:46 - loss: 0.7533 - acc: 0.49 - ETA: 5:00:23 - loss: 0.7522 - acc: 0.49 - ETA: 4:58:34 - loss: 0.7504 - acc: 0.49 - ETA: 4:58:53 - loss: 0.7494 - acc: 0.50 - ETA: 4:58:16 - loss: 0.7484 - acc: 0.50 - ETA: 4:57:49 - loss: 0.7492 - acc: 0.50 - ETA: 4:57:34 - loss: 0.7489 - acc: 0.50 - ETA: 4:57:39 - loss: 0.7494 - acc: 0.50 - ETA: 4:57:42 - loss: 0.7503 - acc: 0.50 - ETA: 4:57:13 - loss: 0.7508 - acc: 0.50 - ETA: 4:56:34 - loss: 0.7501 - acc: 0.49 - ETA: 4:55:56 - loss: 0.7492 - acc: 0.50 - ETA: 4:55:02 - loss: 0.7521 - acc: 0.49 - ETA: 4:54:34 - loss: 0.7522 - acc: 0.49 - ETA: 4:54:18 - loss: 0.7510 - acc: 0.49 - ETA: 4:53:58 - loss: 0.7467 - acc: 0.50 - ETA: 4:54:17 - loss: 0.7495 - acc: 0.50 - ETA: 4:53:48 - loss: 0.7493 - acc: 0.50 - ETA: 4:53:19 - loss: 0.7573 - acc: 0.50 - ETA: 4:53:05 - loss: 0.7568 - acc: 0.50 - ETA: 4:52:45 - loss: 0.7635 - acc: 0.50 - ETA: 4:52:20 - loss: 0.7651 - acc: 0.50 - ETA: 4:52:06 - loss: 0.7631 - acc: 0.50 - ETA: 4:51:41 - loss: 0.7687 - acc: 0.50 - ETA: 4:51:24 - loss: 0.7683 - acc: 0.50 - ETA: 4:50:42 - loss: 0.7681 - acc: 0.50 - ETA: 4:50:21 - loss: 0.7679 - acc: 0.50 - ETA: 4:50:11 - loss: 0.8004 - acc: 0.50 - ETA: 4:49:49 - loss: 0.7992 - acc: 0.50 - ETA: 4:49:19 - loss: 0.7980 - acc: 0.50 - ETA: 4:48:33 - loss: 0.7967 - acc: 0.50 - ETA: 4:48:11 - loss: 0.7961 - acc: 0.50 - ETA: 4:47:59 - loss: 0.7952 - acc: 0.50 - ETA: 4:47:29 - loss: 0.7952 - acc: 0.50 - ETA: 4:47:16 - loss: 0.7937 - acc: 0.50 - ETA: 4:46:52 - loss: 0.7937 - acc: 0.50 - ETA: 4:46:48 - loss: 0.7931 - acc: 0.50 - ETA: 4:46:39 - loss: 0.7922 - acc: 0.50 - ETA: 4:46:20 - loss: 0.7922 - acc: 0.49 - ETA: 4:46:03 - loss: 0.7909 - acc: 0.50 - ETA: 4:45:08 - loss: 0.7899 - acc: 0.50 - ETA: 4:44:25 - loss: 0.7893 - acc: 0.49 - ETA: 4:43:53 - loss: 0.7882 - acc: 0.49 - ETA: 4:43:27 - loss: 0.7867 - acc: 0.50 - ETA: 4:42:58 - loss: 0.7866 - acc: 0.50 - ETA: 4:42:14 - loss: 0.7865 - acc: 0.50 - ETA: 4:41:22 - loss: 0.7860 - acc: 0.50 - ETA: 4:41:09 - loss: 0.7858 - acc: 0.49 - ETA: 4:40:21 - loss: 0.7850 - acc: 0.50 - ETA: 4:39:49 - loss: 0.7840 - acc: 0.50 - ETA: 4:39:36 - loss: 0.7832 - acc: 0.50 - ETA: 4:38:54 - loss: 0.7826 - acc: 0.49 - ETA: 4:38:35 - loss: 0.7822 - acc: 0.49 - ETA: 4:37:56 - loss: 0.7814 - acc: 0.49 - ETA: 4:37:12 - loss: 0.7806 - acc: 0.49 - ETA: 4:36:23 - loss: 0.7802 - acc: 0.49 - ETA: 4:36:01 - loss: 0.7802 - acc: 0.49 - ETA: 4:35:46 - loss: 0.7797 - acc: 0.49 - ETA: 4:35:12 - loss: 0.7790 - acc: 0.49 - ETA: 4:34:27 - loss: 0.7786 - acc: 0.49 - ETA: 4:34:11 - loss: 0.7779 - acc: 0.49 - ETA: 4:33:45 - loss: 0.7771 - acc: 0.49 - ETA: 4:33:18 - loss: 0.7763 - acc: 0.49 - ETA: 4:32:48 - loss: 0.7756 - acc: 0.49 - ETA: 4:32:25 - loss: 0.7751 - acc: 0.49 - ETA: 4:31:59 - loss: 0.7747 - acc: 0.49 - ETA: 4:31:19 - loss: 0.7741 - acc: 0.49 - ETA: 4:30:54 - loss: 0.7733 - acc: 0.49 - ETA: 4:30:26 - loss: 0.7732 - acc: 0.49 - ETA: 4:29:44 - loss: 0.7729 - acc: 0.49 - ETA: 4:29:23 - loss: 0.7725 - acc: 0.49 - ETA: 4:28:54 - loss: 0.7719 - acc: 0.49 - ETA: 4:28:27 - loss: 0.7715 - acc: 0.49 - ETA: 4:28:18 - loss: 0.7711 - acc: 0.49 - ETA: 4:27:57 - loss: 0.7706 - acc: 0.49 - ETA: 4:27:24 - loss: 0.7700 - acc: 0.49 - ETA: 4:27:10 - loss: 0.7694 - acc: 0.49 - ETA: 4:26:37 - loss: 0.7687 - acc: 0.49 - ETA: 4:25:50 - loss: 0.7679 - acc: 0.49 - ETA: 4:25:20 - loss: 0.7674 - acc: 0.49 - ETA: 4:24:46 - loss: 0.7678 - acc: 0.49 - ETA: 4:24:14 - loss: 0.7675 - acc: 0.49 - ETA: 4:23:22 - loss: 0.7670 - acc: 0.49 - ETA: 4:23:07 - loss: 0.7667 - acc: 0.49 - ETA: 4:22:40 - loss: 0.7672 - acc: 0.49 - ETA: 4:22:15 - loss: 0.7668 - acc: 0.49 - ETA: 4:22:08 - loss: 0.7663 - acc: 0.49 - ETA: 4:21:34 - loss: 0.7657 - acc: 0.49 - ETA: 4:21:15 - loss: 0.7652 - acc: 0.49 - ETA: 4:20:57 - loss: 0.7648 - acc: 0.49 - ETA: 4:20:42 - loss: 0.7642 - acc: 0.49 - ETA: 4:20:18 - loss: 0.7637 - acc: 0.49 - ETA: 4:19:35 - loss: 0.7635 - acc: 0.49 - ETA: 4:19:04 - loss: 0.7630 - acc: 0.49 - ETA: 4:18:42 - loss: 0.7629 - acc: 0.49 - ETA: 4:18:13 - loss: 0.7625 - acc: 0.49 - ETA: 4:18:05 - loss: 0.7625 - acc: 0.49 - ETA: 4:17:43 - loss: 0.7621 - acc: 0.49 - ETA: 4:17:16 - loss: 0.7617 - acc: 0.49 - ETA: 4:16:52 - loss: 0.7613 - acc: 0.49 - ETA: 4:16:10 - loss: 0.7617 - acc: 0.49 - ETA: 4:15:40 - loss: 0.7610 - acc: 0.49 - ETA: 4:15:17 - loss: 0.7604 - acc: 0.49 - ETA: 4:15:06 - loss: 0.7598 - acc: 0.49 - ETA: 4:14:39 - loss: 0.7594 - acc: 0.49 - ETA: 4:14:17 - loss: 0.7589 - acc: 0.49 - ETA: 4:13:57 - loss: 0.7585 - acc: 0.49 - ETA: 4:13:45 - loss: 0.7582 - acc: 0.49 - ETA: 4:13:20 - loss: 0.7578 - acc: 0.49 - ETA: 4:12:45 - loss: 0.7573 - acc: 0.49 - ETA: 4:12:15 - loss: 0.7571 - acc: 0.49 - ETA: 4:11:56 - loss: 0.7570 - acc: 0.49 - ETA: 4:11:40 - loss: 0.7566 - acc: 0.49 - ETA: 4:11:22 - loss: 0.7563 - acc: 0.49 - ETA: 4:10:44 - loss: 0.7560 - acc: 0.49 - ETA: 4:10:12 - loss: 0.7556 - acc: 0.49 - ETA: 4:09:50 - loss: 0.7555 - acc: 0.49 - ETA: 4:09:21 - loss: 0.7552 - acc: 0.49 - ETA: 4:08:54 - loss: 0.7550 - acc: 0.49 - ETA: 4:08:34 - loss: 0.7546 - acc: 0.49 - ETA: 4:07:56 - loss: 0.7542 - acc: 0.49 - ETA: 4:07:37 - loss: 0.7539 - acc: 0.49 - ETA: 4:07:08 - loss: 0.7535 - acc: 0.49 - ETA: 4:06:44 - loss: 0.7531 - acc: 0.49 - ETA: 4:06:26 - loss: 0.7528 - acc: 0.49 - ETA: 4:05:59 - loss: 0.7528 - acc: 0.49 - ETA: 4:05:36 - loss: 0.7523 - acc: 0.49 - ETA: 4:05:25 - loss: 0.7520 - acc: 0.49 - ETA: 4:05:04 - loss: 0.7517 - acc: 0.49 - ETA: 4:04:35 - loss: 0.7514 - acc: 0.49 - ETA: 4:04:10 - loss: 0.7511 - acc: 0.49 - ETA: 4:03:39 - loss: 0.7506 - acc: 0.49 - ETA: 4:03:10 - loss: 0.7505 - acc: 0.49 - ETA: 4:02:51 - loss: 0.7509 - acc: 0.49 - ETA: 4:02:34 - loss: 0.7506 - acc: 0.49 - ETA: 4:02:04 - loss: 0.7502 - acc: 0.49 - ETA: 4:01:42 - loss: 0.7501 - acc: 0.49 - ETA: 4:01:12 - loss: 0.7501 - acc: 0.49 - ETA: 4:00:51 - loss: 0.7498 - acc: 0.49 - ETA: 4:00:37 - loss: 0.7495 - acc: 0.49 - ETA: 4:00:16 - loss: 0.7494 - acc: 0.49 - ETA: 3:59:53 - loss: 0.7492 - acc: 0.49 - ETA: 3:59:33 - loss: 0.7490 - acc: 0.49 - ETA: 3:59:05 - loss: 0.7487 - acc: 0.494012416/25000 [=============>................] - ETA: 3:58:30 - loss: 0.7485 - acc: 0.49 - ETA: 3:58:02 - loss: 0.7482 - acc: 0.49 - ETA: 3:57:36 - loss: 0.7479 - acc: 0.49 - ETA: 3:57:18 - loss: 0.7476 - acc: 0.49 - ETA: 3:57:04 - loss: 0.7474 - acc: 0.49 - ETA: 3:56:52 - loss: 0.7471 - acc: 0.49 - ETA: 3:56:33 - loss: 0.7468 - acc: 0.49 - ETA: 3:56:16 - loss: 0.7465 - acc: 0.49 - ETA: 3:55:51 - loss: 0.7463 - acc: 0.49 - ETA: 3:55:29 - loss: 0.7461 - acc: 0.49 - ETA: 3:55:12 - loss: 0.7458 - acc: 0.49 - ETA: 3:54:51 - loss: 0.7456 - acc: 0.49 - ETA: 3:54:29 - loss: 0.7453 - acc: 0.49 - ETA: 3:54:13 - loss: 0.7450 - acc: 0.49 - ETA: 3:54:03 - loss: 0.7448 - acc: 0.49 - ETA: 3:53:38 - loss: 0.7447 - acc: 0.49 - ETA: 3:53:01 - loss: 0.7444 - acc: 0.49 - ETA: 3:52:29 - loss: 0.7443 - acc: 0.49 - ETA: 3:51:59 - loss: 0.7440 - acc: 0.49 - ETA: 3:51:33 - loss: 0.7438 - acc: 0.49 - ETA: 3:51:13 - loss: 0.7435 - acc: 0.49 - ETA: 3:50:50 - loss: 0.7434 - acc: 0.49 - ETA: 3:50:26 - loss: 0.7433 - acc: 0.49 - ETA: 3:50:03 - loss: 0.7430 - acc: 0.49 - ETA: 3:49:48 - loss: 0.7428 - acc: 0.49 - ETA: 3:49:16 - loss: 0.7426 - acc: 0.49 - ETA: 3:48:44 - loss: 0.7423 - acc: 0.49 - ETA: 3:48:23 - loss: 0.7421 - acc: 0.49 - ETA: 3:48:02 - loss: 0.7421 - acc: 0.49 - ETA: 3:47:34 - loss: 0.7419 - acc: 0.49 - ETA: 3:47:12 - loss: 0.7417 - acc: 0.49 - ETA: 3:46:51 - loss: 0.7414 - acc: 0.49 - ETA: 3:46:31 - loss: 0.7413 - acc: 0.49 - ETA: 3:46:21 - loss: 0.7412 - acc: 0.49 - ETA: 3:45:49 - loss: 0.7410 - acc: 0.49 - ETA: 3:45:22 - loss: 0.7408 - acc: 0.49 - ETA: 3:44:52 - loss: 0.7406 - acc: 0.49 - ETA: 3:44:28 - loss: 0.7404 - acc: 0.49 - ETA: 3:44:05 - loss: 0.7402 - acc: 0.49 - ETA: 3:43:41 - loss: 0.7400 - acc: 0.49 - ETA: 3:43:11 - loss: 0.7398 - acc: 0.49 - ETA: 3:42:52 - loss: 0.7396 - acc: 0.49 - ETA: 3:42:24 - loss: 0.7394 - acc: 0.49 - ETA: 3:41:59 - loss: 0.7393 - acc: 0.49 - ETA: 3:41:27 - loss: 0.7392 - acc: 0.49 - ETA: 3:41:03 - loss: 0.7390 - acc: 0.49 - ETA: 3:40:38 - loss: 0.7389 - acc: 0.49 - ETA: 3:40:17 - loss: 0.7388 - acc: 0.49 - ETA: 3:39:49 - loss: 0.7386 - acc: 0.49 - ETA: 3:39:27 - loss: 0.7384 - acc: 0.49 - ETA: 3:39:04 - loss: 0.7382 - acc: 0.49 - ETA: 3:38:39 - loss: 0.7380 - acc: 0.49 - ETA: 3:38:12 - loss: 0.7379 - acc: 0.49 - ETA: 3:37:45 - loss: 0.7378 - acc: 0.49 - ETA: 3:37:26 - loss: 0.7375 - acc: 0.49 - ETA: 3:37:06 - loss: 0.7373 - acc: 0.49 - ETA: 3:36:42 - loss: 0.7369 - acc: 0.49 - ETA: 3:36:15 - loss: 0.7366 - acc: 0.49 - ETA: 3:35:50 - loss: 0.7368 - acc: 0.49 - ETA: 3:35:25 - loss: 0.7367 - acc: 0.49 - ETA: 3:34:55 - loss: 0.7366 - acc: 0.49 - ETA: 3:34:31 - loss: 0.7364 - acc: 0.49 - ETA: 3:34:15 - loss: 0.7363 - acc: 0.49 - ETA: 3:33:53 - loss: 0.7361 - acc: 0.49 - ETA: 3:33:29 - loss: 0.7360 - acc: 0.49 - ETA: 3:33:02 - loss: 0.7359 - acc: 0.49 - ETA: 3:32:38 - loss: 0.7358 - acc: 0.49 - ETA: 3:32:14 - loss: 0.7357 - acc: 0.49 - ETA: 3:31:50 - loss: 0.7357 - acc: 0.49 - ETA: 3:31:29 - loss: 0.7355 - acc: 0.49 - ETA: 3:31:04 - loss: 0.7355 - acc: 0.49 - ETA: 3:30:38 - loss: 0.7354 - acc: 0.49 - ETA: 3:30:14 - loss: 0.7352 - acc: 0.49 - ETA: 3:29:49 - loss: 0.7351 - acc: 0.49 - ETA: 3:29:25 - loss: 0.7350 - acc: 0.49 - ETA: 3:28:59 - loss: 0.7349 - acc: 0.49 - ETA: 3:28:35 - loss: 0.7349 - acc: 0.49 - ETA: 3:28:12 - loss: 0.7349 - acc: 0.49 - ETA: 3:27:46 - loss: 0.7347 - acc: 0.49 - ETA: 3:27:13 - loss: 0.7346 - acc: 0.49 - ETA: 3:26:48 - loss: 0.7345 - acc: 0.49 - ETA: 3:26:25 - loss: 0.7341 - acc: 0.49 - ETA: 3:25:55 - loss: 0.7340 - acc: 0.49 - ETA: 3:25:29 - loss: 0.7339 - acc: 0.49 - ETA: 3:25:03 - loss: 0.7341 - acc: 0.49 - ETA: 3:24:39 - loss: 0.7340 - acc: 0.49 - ETA: 3:24:17 - loss: 0.7340 - acc: 0.49 - ETA: 3:23:50 - loss: 0.7339 - acc: 0.49 - ETA: 3:23:29 - loss: 0.7337 - acc: 0.49 - ETA: 3:23:06 - loss: 0.7337 - acc: 0.49 - ETA: 3:22:41 - loss: 0.7335 - acc: 0.49 - ETA: 3:22:16 - loss: 0.7335 - acc: 0.49 - ETA: 3:21:54 - loss: 0.7334 - acc: 0.49 - ETA: 3:21:29 - loss: 0.7333 - acc: 0.49 - ETA: 3:21:07 - loss: 0.7332 - acc: 0.49 - ETA: 3:20:43 - loss: 0.7331 - acc: 0.49 - ETA: 3:20:22 - loss: 0.7328 - acc: 0.49 - ETA: 3:19:58 - loss: 0.7328 - acc: 0.49 - ETA: 3:19:31 - loss: 0.7326 - acc: 0.49 - ETA: 3:19:08 - loss: 0.7328 - acc: 0.49 - ETA: 3:18:46 - loss: 0.7330 - acc: 0.49 - ETA: 3:18:22 - loss: 0.7330 - acc: 0.49 - ETA: 3:17:51 - loss: 0.7330 - acc: 0.49 - ETA: 3:17:25 - loss: 0.7329 - acc: 0.49 - ETA: 3:17:02 - loss: 0.7327 - acc: 0.49 - ETA: 3:16:34 - loss: 0.7326 - acc: 0.49 - ETA: 3:16:16 - loss: 0.7325 - acc: 0.49 - ETA: 3:15:51 - loss: 0.7326 - acc: 0.49 - ETA: 3:15:30 - loss: 0.7327 - acc: 0.49 - ETA: 3:15:04 - loss: 0.7327 - acc: 0.49 - ETA: 3:14:41 - loss: 0.7326 - acc: 0.49 - ETA: 3:14:16 - loss: 0.7324 - acc: 0.49 - ETA: 3:13:53 - loss: 0.7323 - acc: 0.49 - ETA: 3:13:30 - loss: 0.7322 - acc: 0.49 - ETA: 3:13:03 - loss: 0.7321 - acc: 0.49 - ETA: 3:12:34 - loss: 0.7320 - acc: 0.49 - ETA: 3:12:09 - loss: 0.7319 - acc: 0.49 - ETA: 3:11:41 - loss: 0.7318 - acc: 0.49 - ETA: 3:11:17 - loss: 0.7317 - acc: 0.49 - ETA: 3:10:53 - loss: 0.7316 - acc: 0.49 - ETA: 3:10:24 - loss: 0.7315 - acc: 0.49 - ETA: 3:10:04 - loss: 0.7315 - acc: 0.49 - ETA: 3:09:37 - loss: 0.7314 - acc: 0.49 - ETA: 3:09:14 - loss: 0.7312 - acc: 0.49 - ETA: 3:08:49 - loss: 0.7311 - acc: 0.49 - ETA: 3:08:21 - loss: 0.7310 - acc: 0.49 - ETA: 3:07:58 - loss: 0.7309 - acc: 0.49 - ETA: 3:07:32 - loss: 0.7308 - acc: 0.49 - ETA: 3:07:08 - loss: 0.7307 - acc: 0.49 - ETA: 3:06:45 - loss: 0.7306 - acc: 0.49 - ETA: 3:06:19 - loss: 0.7304 - acc: 0.49 - ETA: 3:05:56 - loss: 0.7303 - acc: 0.49 - ETA: 3:05:33 - loss: 0.7302 - acc: 0.49 - ETA: 3:05:12 - loss: 0.7303 - acc: 0.49 - ETA: 3:04:49 - loss: 0.7302 - acc: 0.49 - ETA: 3:04:21 - loss: 0.7302 - acc: 0.49 - ETA: 3:03:53 - loss: 0.7301 - acc: 0.49 - ETA: 3:03:27 - loss: 0.7300 - acc: 0.49 - ETA: 3:03:02 - loss: 0.7300 - acc: 0.49 - ETA: 3:02:36 - loss: 0.7298 - acc: 0.49 - ETA: 3:02:11 - loss: 0.7300 - acc: 0.49 - ETA: 3:01:47 - loss: 0.7298 - acc: 0.49 - ETA: 3:01:20 - loss: 0.7297 - acc: 0.49 - ETA: 3:00:56 - loss: 0.7296 - acc: 0.49 - ETA: 3:00:34 - loss: 0.7295 - acc: 0.49 - ETA: 3:00:09 - loss: 0.7293 - acc: 0.49 - ETA: 2:59:43 - loss: 0.7291 - acc: 0.49 - ETA: 2:59:18 - loss: 0.7290 - acc: 0.49 - ETA: 2:58:54 - loss: 0.7289 - acc: 0.49 - ETA: 2:58:31 - loss: 0.7287 - acc: 0.49 - ETA: 2:58:09 - loss: 0.7286 - acc: 0.49 - ETA: 2:57:44 - loss: 0.7286 - acc: 0.49 - ETA: 2:57:21 - loss: 0.7286 - acc: 0.49 - ETA: 2:56:53 - loss: 0.7285 - acc: 0.49 - ETA: 2:56:30 - loss: 0.7284 - acc: 0.49 - ETA: 2:56:07 - loss: 0.7283 - acc: 0.49 - ETA: 2:55:41 - loss: 0.7282 - acc: 0.49 - ETA: 2:55:16 - loss: 0.7283 - acc: 0.49 - ETA: 2:54:50 - loss: 0.7282 - acc: 0.49 - ETA: 2:54:25 - loss: 0.7281 - acc: 0.49 - ETA: 2:54:01 - loss: 0.7281 - acc: 0.49 - ETA: 2:53:34 - loss: 0.7282 - acc: 0.49 - ETA: 2:53:06 - loss: 0.7282 - acc: 0.49 - ETA: 2:52:41 - loss: 0.7281 - acc: 0.49 - ETA: 2:52:18 - loss: 0.7280 - acc: 0.49 - ETA: 2:51:57 - loss: 0.7279 - acc: 0.49 - ETA: 2:51:34 - loss: 0.7278 - acc: 0.49 - ETA: 2:51:09 - loss: 0.7277 - acc: 0.49 - ETA: 2:50:43 - loss: 0.7276 - acc: 0.49 - ETA: 2:50:17 - loss: 0.7275 - acc: 0.49 - ETA: 2:49:48 - loss: 0.7274 - acc: 0.49 - ETA: 2:49:26 - loss: 0.7274 - acc: 0.49 - ETA: 2:48:57 - loss: 0.7273 - acc: 0.49 - ETA: 2:48:34 - loss: 0.7272 - acc: 0.49 - ETA: 2:48:11 - loss: 0.7271 - acc: 0.49 - ETA: 2:47:43 - loss: 0.7270 - acc: 0.49 - ETA: 2:47:19 - loss: 0.7269 - acc: 0.49 - ETA: 2:46:51 - loss: 0.7268 - acc: 0.49 - ETA: 2:46:27 - loss: 0.7267 - acc: 0.49 - ETA: 2:46:03 - loss: 0.7266 - acc: 0.49 - ETA: 2:45:41 - loss: 0.7264 - acc: 0.49 - ETA: 2:45:20 - loss: 0.7265 - acc: 0.49 - ETA: 2:44:57 - loss: 0.7264 - acc: 0.49 - ETA: 2:44:36 - loss: 0.7263 - acc: 0.49 - ETA: 2:44:10 - loss: 0.7262 - acc: 0.49 - ETA: 2:43:47 - loss: 0.7261 - acc: 0.49 - ETA: 2:43:22 - loss: 0.7261 - acc: 0.49 - ETA: 2:42:57 - loss: 0.7260 - acc: 0.49 - ETA: 2:42:30 - loss: 0.7259 - acc: 0.49 - ETA: 2:42:07 - loss: 0.7259 - acc: 0.49 - ETA: 2:41:42 - loss: 0.7258 - acc: 0.49 - ETA: 2:41:17 - loss: 0.7257 - acc: 0.49 - ETA: 2:40:54 - loss: 0.7256 - acc: 0.49 - ETA: 2:40:31 - loss: 0.7256 - acc: 0.495718624/25000 [=====================>........] - ETA: 2:40:04 - loss: 0.7255 - acc: 0.49 - ETA: 2:39:44 - loss: 0.7254 - acc: 0.49 - ETA: 2:39:18 - loss: 0.7253 - acc: 0.49 - ETA: 2:38:58 - loss: 0.7252 - acc: 0.49 - ETA: 2:38:38 - loss: 0.7252 - acc: 0.49 - ETA: 2:38:18 - loss: 0.7252 - acc: 0.49 - ETA: 2:37:55 - loss: 0.7250 - acc: 0.49 - ETA: 2:37:30 - loss: 0.7250 - acc: 0.49 - ETA: 2:37:07 - loss: 0.7250 - acc: 0.49 - ETA: 2:36:45 - loss: 0.7249 - acc: 0.49 - ETA: 2:36:22 - loss: 0.7248 - acc: 0.49 - ETA: 2:36:04 - loss: 0.7247 - acc: 0.49 - ETA: 2:35:41 - loss: 0.7246 - acc: 0.49 - ETA: 2:35:19 - loss: 0.7246 - acc: 0.49 - ETA: 2:34:55 - loss: 0.7245 - acc: 0.49 - ETA: 2:34:34 - loss: 0.7243 - acc: 0.49 - ETA: 2:34:14 - loss: 0.7242 - acc: 0.49 - ETA: 2:33:56 - loss: 0.7240 - acc: 0.49 - ETA: 2:33:35 - loss: 0.7238 - acc: 0.49 - ETA: 2:33:11 - loss: 0.7238 - acc: 0.49 - ETA: 2:32:49 - loss: 0.7239 - acc: 0.49 - ETA: 2:32:21 - loss: 0.7237 - acc: 0.49 - ETA: 2:32:00 - loss: 0.7236 - acc: 0.49 - ETA: 2:31:36 - loss: 0.7237 - acc: 0.49 - ETA: 2:31:13 - loss: 0.7235 - acc: 0.49 - ETA: 2:30:48 - loss: 0.7234 - acc: 0.49 - ETA: 2:30:22 - loss: 0.7234 - acc: 0.49 - ETA: 2:29:58 - loss: 0.7234 - acc: 0.49 - ETA: 2:29:32 - loss: 0.7233 - acc: 0.49 - ETA: 2:29:06 - loss: 0.7232 - acc: 0.49 - ETA: 2:28:44 - loss: 0.7232 - acc: 0.49 - ETA: 2:28:23 - loss: 0.7231 - acc: 0.49 - ETA: 2:27:55 - loss: 0.7230 - acc: 0.49 - ETA: 2:27:29 - loss: 0.7230 - acc: 0.49 - ETA: 2:27:02 - loss: 0.7229 - acc: 0.49 - ETA: 2:26:38 - loss: 0.7229 - acc: 0.49 - ETA: 2:26:13 - loss: 0.7228 - acc: 0.49 - ETA: 2:25:49 - loss: 0.7228 - acc: 0.49 - ETA: 2:25:27 - loss: 0.7227 - acc: 0.49 - ETA: 2:25:05 - loss: 0.7227 - acc: 0.49 - ETA: 2:24:39 - loss: 0.7225 - acc: 0.49 - ETA: 2:24:12 - loss: 0.7224 - acc: 0.49 - ETA: 2:23:48 - loss: 0.7224 - acc: 0.49 - ETA: 2:23:23 - loss: 0.7224 - acc: 0.49 - ETA: 2:22:59 - loss: 0.7223 - acc: 0.49 - ETA: 2:22:33 - loss: 0.7223 - acc: 0.49 - ETA: 2:22:09 - loss: 0.7222 - acc: 0.49 - ETA: 2:21:44 - loss: 0.7223 - acc: 0.49 - ETA: 2:21:20 - loss: 0.7223 - acc: 0.49 - ETA: 2:20:58 - loss: 0.7222 - acc: 0.49 - ETA: 2:20:33 - loss: 0.7221 - acc: 0.49 - ETA: 2:20:04 - loss: 0.7222 - acc: 0.49 - ETA: 2:19:42 - loss: 0.7221 - acc: 0.49 - ETA: 2:19:17 - loss: 0.7221 - acc: 0.49 - ETA: 2:18:56 - loss: 0.7221 - acc: 0.49 - ETA: 2:18:33 - loss: 0.7221 - acc: 0.49 - ETA: 2:18:11 - loss: 0.7220 - acc: 0.49 - ETA: 2:17:45 - loss: 0.7220 - acc: 0.49 - ETA: 2:17:19 - loss: 0.7219 - acc: 0.49 - ETA: 2:16:54 - loss: 0.7218 - acc: 0.49 - ETA: 2:16:31 - loss: 0.7218 - acc: 0.49 - ETA: 2:16:06 - loss: 0.7217 - acc: 0.49 - ETA: 2:15:42 - loss: 0.7217 - acc: 0.49 - ETA: 2:15:19 - loss: 0.7216 - acc: 0.49 - ETA: 2:14:55 - loss: 0.7216 - acc: 0.49 - ETA: 2:14:30 - loss: 0.7215 - acc: 0.49 - ETA: 2:14:04 - loss: 0.7215 - acc: 0.49 - ETA: 2:13:39 - loss: 0.7215 - acc: 0.49 - ETA: 2:13:12 - loss: 0.7215 - acc: 0.49 - ETA: 2:12:47 - loss: 0.7214 - acc: 0.49 - ETA: 2:12:22 - loss: 0.7213 - acc: 0.49 - ETA: 2:11:57 - loss: 0.7212 - acc: 0.49 - ETA: 2:11:31 - loss: 0.7213 - acc: 0.49 - ETA: 2:11:07 - loss: 0.7212 - acc: 0.49 - ETA: 2:10:41 - loss: 0.7212 - acc: 0.49 - ETA: 2:10:17 - loss: 0.7211 - acc: 0.49 - ETA: 2:09:50 - loss: 0.7211 - acc: 0.49 - ETA: 2:09:27 - loss: 0.7211 - acc: 0.49 - ETA: 2:09:03 - loss: 0.7210 - acc: 0.49 - ETA: 2:08:39 - loss: 0.7213 - acc: 0.49 - ETA: 2:08:11 - loss: 0.7212 - acc: 0.49 - ETA: 2:07:48 - loss: 0.7212 - acc: 0.49 - ETA: 2:07:26 - loss: 0.7211 - acc: 0.49 - ETA: 2:07:07 - loss: 0.7210 - acc: 0.49 - ETA: 2:06:45 - loss: 0.7209 - acc: 0.49 - ETA: 2:06:25 - loss: 0.7211 - acc: 0.49 - ETA: 2:06:00 - loss: 0.7210 - acc: 0.49 - ETA: 2:05:35 - loss: 0.7212 - acc: 0.49 - ETA: 2:05:14 - loss: 0.7214 - acc: 0.49 - ETA: 2:04:50 - loss: 0.7213 - acc: 0.49 - ETA: 2:04:28 - loss: 0.7213 - acc: 0.49 - ETA: 2:04:03 - loss: 0.7212 - acc: 0.49 - ETA: 2:03:40 - loss: 0.7210 - acc: 0.49 - ETA: 2:03:18 - loss: 0.7211 - acc: 0.49 - ETA: 2:02:57 - loss: 0.7213 - acc: 0.49 - ETA: 2:02:35 - loss: 0.7213 - acc: 0.49 - ETA: 2:02:10 - loss: 0.7213 - acc: 0.49 - ETA: 2:01:47 - loss: 0.7212 - acc: 0.49 - ETA: 2:01:24 - loss: 0.7212 - acc: 0.49 - ETA: 2:01:00 - loss: 0.7212 - acc: 0.49 - ETA: 2:00:36 - loss: 0.7211 - acc: 0.49 - ETA: 2:00:11 - loss: 0.7212 - acc: 0.49 - ETA: 1:59:47 - loss: 0.7211 - acc: 0.49 - ETA: 1:59:24 - loss: 0.7211 - acc: 0.49 - ETA: 1:59:02 - loss: 0.7210 - acc: 0.49 - ETA: 1:58:36 - loss: 0.7209 - acc: 0.49 - ETA: 1:58:12 - loss: 0.7209 - acc: 0.49 - ETA: 1:57:46 - loss: 0.7208 - acc: 0.49 - ETA: 1:57:21 - loss: 0.7207 - acc: 0.49 - ETA: 1:56:57 - loss: 0.7206 - acc: 0.49 - ETA: 1:56:34 - loss: 0.7206 - acc: 0.49 - ETA: 1:56:09 - loss: 0.7206 - acc: 0.49 - ETA: 1:55:44 - loss: 0.7206 - acc: 0.49 - ETA: 1:55:20 - loss: 0.7206 - acc: 0.49 - ETA: 1:54:55 - loss: 0.7205 - acc: 0.49 - ETA: 1:54:32 - loss: 0.7205 - acc: 0.49 - ETA: 1:54:07 - loss: 0.7204 - acc: 0.49 - ETA: 1:53:42 - loss: 0.7203 - acc: 0.49 - ETA: 1:53:17 - loss: 0.7203 - acc: 0.49 - ETA: 1:52:52 - loss: 0.7203 - acc: 0.49 - ETA: 1:52:28 - loss: 0.7202 - acc: 0.49 - ETA: 1:52:03 - loss: 0.7202 - acc: 0.49 - ETA: 1:51:39 - loss: 0.7202 - acc: 0.49 - ETA: 1:51:13 - loss: 0.7201 - acc: 0.49 - ETA: 1:50:49 - loss: 0.7201 - acc: 0.49 - ETA: 1:50:22 - loss: 0.7201 - acc: 0.49 - ETA: 1:49:57 - loss: 0.7200 - acc: 0.49 - ETA: 1:49:34 - loss: 0.7200 - acc: 0.49 - ETA: 1:49:09 - loss: 0.7200 - acc: 0.49 - ETA: 1:48:44 - loss: 0.7199 - acc: 0.49 - ETA: 1:48:20 - loss: 0.7199 - acc: 0.49 - ETA: 1:47:55 - loss: 0.7198 - acc: 0.49 - ETA: 1:47:31 - loss: 0.7198 - acc: 0.49 - ETA: 1:47:07 - loss: 0.7197 - acc: 0.49 - ETA: 1:46:43 - loss: 0.7197 - acc: 0.49 - ETA: 1:46:19 - loss: 0.7196 - acc: 0.49 - ETA: 1:45:53 - loss: 0.7196 - acc: 0.49 - ETA: 1:45:28 - loss: 0.7195 - acc: 0.49 - ETA: 1:45:04 - loss: 0.7194 - acc: 0.49 - ETA: 1:44:38 - loss: 0.7193 - acc: 0.49 - ETA: 1:44:12 - loss: 0.7193 - acc: 0.49 - ETA: 1:43:46 - loss: 0.7192 - acc: 0.49 - ETA: 1:43:20 - loss: 0.7192 - acc: 0.49 - ETA: 1:42:55 - loss: 0.7193 - acc: 0.49 - ETA: 1:42:30 - loss: 0.7192 - acc: 0.49 - ETA: 1:42:07 - loss: 0.7192 - acc: 0.49 - ETA: 1:41:42 - loss: 0.7192 - acc: 0.49 - ETA: 1:41:16 - loss: 0.7191 - acc: 0.49 - ETA: 1:40:53 - loss: 0.7190 - acc: 0.49 - ETA: 1:40:29 - loss: 0.7191 - acc: 0.49 - ETA: 1:40:04 - loss: 0.7192 - acc: 0.49 - ETA: 1:39:41 - loss: 0.7193 - acc: 0.49 - ETA: 1:39:15 - loss: 0.7191 - acc: 0.49 - ETA: 1:38:51 - loss: 0.7192 - acc: 0.49 - ETA: 1:38:25 - loss: 0.7191 - acc: 0.49 - ETA: 1:38:02 - loss: 0.7191 - acc: 0.49 - ETA: 1:37:38 - loss: 0.7190 - acc: 0.49 - ETA: 1:37:13 - loss: 0.7191 - acc: 0.49 - ETA: 1:36:49 - loss: 0.7190 - acc: 0.49 - ETA: 1:36:25 - loss: 0.7190 - acc: 0.49 - ETA: 1:35:59 - loss: 0.7189 - acc: 0.49 - ETA: 1:35:34 - loss: 0.7189 - acc: 0.49 - ETA: 1:35:09 - loss: 0.7189 - acc: 0.49 - ETA: 1:34:44 - loss: 0.7189 - acc: 0.49 - ETA: 1:34:19 - loss: 0.7188 - acc: 0.49 - ETA: 1:33:53 - loss: 0.7188 - acc: 0.49 - ETA: 1:33:27 - loss: 0.7187 - acc: 0.49 - ETA: 1:33:02 - loss: 0.7187 - acc: 0.49 - ETA: 1:32:37 - loss: 0.7187 - acc: 0.49 - ETA: 1:32:13 - loss: 0.7187 - acc: 0.49 - ETA: 1:31:49 - loss: 0.7186 - acc: 0.49 - ETA: 1:31:23 - loss: 0.7186 - acc: 0.49 - ETA: 1:30:59 - loss: 0.7188 - acc: 0.49 - ETA: 1:30:34 - loss: 0.7189 - acc: 0.49 - ETA: 1:30:09 - loss: 0.7189 - acc: 0.49 - ETA: 1:29:44 - loss: 0.7188 - acc: 0.49 - ETA: 1:29:19 - loss: 0.7189 - acc: 0.49 - ETA: 1:28:55 - loss: 0.7188 - acc: 0.49 - ETA: 1:28:30 - loss: 0.7188 - acc: 0.49 - ETA: 1:28:05 - loss: 0.7187 - acc: 0.49 - ETA: 1:27:40 - loss: 0.7187 - acc: 0.49 - ETA: 1:27:16 - loss: 0.7186 - acc: 0.49 - ETA: 1:26:52 - loss: 0.7186 - acc: 0.49 - ETA: 1:26:27 - loss: 0.7186 - acc: 0.49 - ETA: 1:26:03 - loss: 0.7185 - acc: 0.49 - ETA: 1:25:39 - loss: 0.7185 - acc: 0.49 - ETA: 1:25:14 - loss: 0.7185 - acc: 0.49 - ETA: 1:24:48 - loss: 0.7184 - acc: 0.49 - ETA: 1:24:24 - loss: 0.7184 - acc: 0.49 - ETA: 1:23:59 - loss: 0.7183 - acc: 0.49 - ETA: 1:23:35 - loss: 0.7183 - acc: 0.49 - ETA: 1:23:10 - loss: 0.7183 - acc: 0.49 - ETA: 1:22:45 - loss: 0.7182 - acc: 0.49 - ETA: 1:22:21 - loss: 0.7183 - acc: 0.4969"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 1:21:58 - loss: 0.7182 - acc: 0.49 - ETA: 1:21:35 - loss: 0.7183 - acc: 0.49 - ETA: 1:21:12 - loss: 0.7183 - acc: 0.49 - ETA: 1:20:47 - loss: 0.7184 - acc: 0.49 - ETA: 1:20:22 - loss: 0.7184 - acc: 0.49 - ETA: 1:19:58 - loss: 0.7185 - acc: 0.49 - ETA: 1:19:34 - loss: 0.7184 - acc: 0.49 - ETA: 1:19:10 - loss: 0.7184 - acc: 0.49 - ETA: 1:18:46 - loss: 0.7184 - acc: 0.49 - ETA: 1:18:20 - loss: 0.7183 - acc: 0.49 - ETA: 1:17:56 - loss: 0.7183 - acc: 0.49 - ETA: 1:17:32 - loss: 0.7183 - acc: 0.49 - ETA: 1:17:07 - loss: 0.7183 - acc: 0.49 - ETA: 1:16:42 - loss: 0.7183 - acc: 0.49 - ETA: 1:16:17 - loss: 0.7182 - acc: 0.49 - ETA: 1:15:54 - loss: 0.7181 - acc: 0.49 - ETA: 1:15:29 - loss: 0.7181 - acc: 0.49 - ETA: 1:15:05 - loss: 0.7181 - acc: 0.49 - ETA: 1:14:39 - loss: 0.7180 - acc: 0.49 - ETA: 1:14:13 - loss: 0.7180 - acc: 0.49 - ETA: 1:13:49 - loss: 0.7179 - acc: 0.49 - ETA: 1:13:24 - loss: 0.7179 - acc: 0.49 - ETA: 1:13:00 - loss: 0.7179 - acc: 0.49 - ETA: 1:12:34 - loss: 0.7178 - acc: 0.49 - ETA: 1:12:09 - loss: 0.7178 - acc: 0.49 - ETA: 1:11:44 - loss: 0.7177 - acc: 0.49 - ETA: 1:11:19 - loss: 0.7177 - acc: 0.49 - ETA: 1:10:54 - loss: 0.7177 - acc: 0.49 - ETA: 1:10:29 - loss: 0.7176 - acc: 0.49 - ETA: 1:10:04 - loss: 0.7176 - acc: 0.49 - ETA: 1:09:38 - loss: 0.7175 - acc: 0.49 - ETA: 1:09:13 - loss: 0.7175 - acc: 0.49 - ETA: 1:08:48 - loss: 0.7175 - acc: 0.49 - ETA: 1:08:24 - loss: 0.7174 - acc: 0.49 - ETA: 1:07:59 - loss: 0.7174 - acc: 0.49 - ETA: 1:07:34 - loss: 0.7173 - acc: 0.49 - ETA: 1:07:10 - loss: 0.7173 - acc: 0.49 - ETA: 1:06:44 - loss: 0.7173 - acc: 0.49 - ETA: 1:06:19 - loss: 0.7172 - acc: 0.49 - ETA: 1:05:54 - loss: 0.7172 - acc: 0.49 - ETA: 1:05:30 - loss: 0.7171 - acc: 0.49 - ETA: 1:05:05 - loss: 0.7172 - acc: 0.49 - ETA: 1:04:41 - loss: 0.7171 - acc: 0.49 - ETA: 1:04:17 - loss: 0.7171 - acc: 0.49 - ETA: 1:03:52 - loss: 0.7171 - acc: 0.49 - ETA: 1:03:27 - loss: 0.7170 - acc: 0.49 - ETA: 1:03:03 - loss: 0.7170 - acc: 0.49 - ETA: 1:02:37 - loss: 0.7170 - acc: 0.49 - ETA: 1:02:13 - loss: 0.7169 - acc: 0.49 - ETA: 1:01:48 - loss: 0.7168 - acc: 0.49 - ETA: 1:01:23 - loss: 0.7168 - acc: 0.49 - ETA: 1:00:58 - loss: 0.7168 - acc: 0.49 - ETA: 1:00:33 - loss: 0.7168 - acc: 0.49 - ETA: 1:00:08 - loss: 0.7168 - acc: 0.49 - ETA: 59:43 - loss: 0.7168 - acc: 0.4973 - ETA: 59:19 - loss: 0.7167 - acc: 0.49 - ETA: 58:54 - loss: 0.7167 - acc: 0.49 - ETA: 58:30 - loss: 0.7167 - acc: 0.49 - ETA: 58:05 - loss: 0.7166 - acc: 0.49 - ETA: 57:41 - loss: 0.7166 - acc: 0.49 - ETA: 57:16 - loss: 0.7165 - acc: 0.49 - ETA: 56:52 - loss: 0.7166 - acc: 0.49 - ETA: 56:28 - loss: 0.7165 - acc: 0.49 - ETA: 56:03 - loss: 0.7165 - acc: 0.49 - ETA: 55:38 - loss: 0.7164 - acc: 0.49 - ETA: 55:13 - loss: 0.7164 - acc: 0.49 - ETA: 54:48 - loss: 0.7163 - acc: 0.49 - ETA: 54:24 - loss: 0.7163 - acc: 0.49 - ETA: 53:59 - loss: 0.7162 - acc: 0.49 - ETA: 53:34 - loss: 0.7162 - acc: 0.49 - ETA: 53:09 - loss: 0.7162 - acc: 0.49 - ETA: 52:45 - loss: 0.7161 - acc: 0.49 - ETA: 52:20 - loss: 0.7161 - acc: 0.49 - ETA: 51:55 - loss: 0.7160 - acc: 0.49 - ETA: 51:30 - loss: 0.7160 - acc: 0.49 - ETA: 51:05 - loss: 0.7159 - acc: 0.49 - ETA: 50:40 - loss: 0.7159 - acc: 0.49 - ETA: 50:15 - loss: 0.7159 - acc: 0.49 - ETA: 49:50 - loss: 0.7158 - acc: 0.49 - ETA: 49:25 - loss: 0.7158 - acc: 0.49 - ETA: 48:59 - loss: 0.7157 - acc: 0.49 - ETA: 48:34 - loss: 0.7157 - acc: 0.49 - ETA: 48:10 - loss: 0.7157 - acc: 0.49 - ETA: 47:45 - loss: 0.7157 - acc: 0.49 - ETA: 47:20 - loss: 0.7157 - acc: 0.49 - ETA: 46:55 - loss: 0.7156 - acc: 0.49 - ETA: 46:29 - loss: 0.7156 - acc: 0.49 - ETA: 46:05 - loss: 0.7156 - acc: 0.49 - ETA: 45:39 - loss: 0.7155 - acc: 0.49 - ETA: 45:14 - loss: 0.7155 - acc: 0.49 - ETA: 44:49 - loss: 0.7155 - acc: 0.49 - ETA: 44:25 - loss: 0.7154 - acc: 0.49 - ETA: 44:00 - loss: 0.7154 - acc: 0.49 - ETA: 43:35 - loss: 0.7154 - acc: 0.49 - ETA: 43:10 - loss: 0.7154 - acc: 0.49 - ETA: 42:44 - loss: 0.7154 - acc: 0.49 - ETA: 42:19 - loss: 0.7153 - acc: 0.49 - ETA: 41:55 - loss: 0.7153 - acc: 0.49 - ETA: 41:29 - loss: 0.7152 - acc: 0.49 - ETA: 41:05 - loss: 0.7152 - acc: 0.49 - ETA: 40:40 - loss: 0.7152 - acc: 0.49 - ETA: 40:15 - loss: 0.7151 - acc: 0.49 - ETA: 39:51 - loss: 0.7151 - acc: 0.49 - ETA: 39:26 - loss: 0.7151 - acc: 0.49 - ETA: 39:00 - loss: 0.7151 - acc: 0.49 - ETA: 38:35 - loss: 0.7150 - acc: 0.49 - ETA: 38:11 - loss: 0.7150 - acc: 0.49 - ETA: 37:46 - loss: 0.7149 - acc: 0.49 - ETA: 37:21 - loss: 0.7149 - acc: 0.49 - ETA: 36:56 - loss: 0.7148 - acc: 0.49 - ETA: 36:31 - loss: 0.7148 - acc: 0.49 - ETA: 36:06 - loss: 0.7148 - acc: 0.49 - ETA: 35:41 - loss: 0.7148 - acc: 0.49 - ETA: 35:16 - loss: 0.7148 - acc: 0.49 - ETA: 34:51 - loss: 0.7148 - acc: 0.49 - ETA: 34:26 - loss: 0.7147 - acc: 0.49 - ETA: 34:01 - loss: 0.7147 - acc: 0.49 - ETA: 33:37 - loss: 0.7146 - acc: 0.49 - ETA: 33:12 - loss: 0.7146 - acc: 0.49 - ETA: 32:47 - loss: 0.7146 - acc: 0.49 - ETA: 32:22 - loss: 0.7146 - acc: 0.49 - ETA: 31:57 - loss: 0.7146 - acc: 0.49 - ETA: 31:32 - loss: 0.7146 - acc: 0.49 - ETA: 31:08 - loss: 0.7145 - acc: 0.49 - ETA: 30:43 - loss: 0.7146 - acc: 0.49 - ETA: 30:18 - loss: 0.7146 - acc: 0.49 - ETA: 29:54 - loss: 0.7146 - acc: 0.49 - ETA: 29:29 - loss: 0.7146 - acc: 0.49 - ETA: 29:04 - loss: 0.7146 - acc: 0.49 - ETA: 28:39 - loss: 0.7145 - acc: 0.49 - ETA: 28:14 - loss: 0.7145 - acc: 0.49 - ETA: 27:50 - loss: 0.7145 - acc: 0.49 - ETA: 27:25 - loss: 0.7145 - acc: 0.49 - ETA: 27:00 - loss: 0.7145 - acc: 0.49 - ETA: 26:35 - loss: 0.7145 - acc: 0.49 - ETA: 26:10 - loss: 0.7145 - acc: 0.49 - ETA: 25:45 - loss: 0.7145 - acc: 0.49 - ETA: 25:20 - loss: 0.7144 - acc: 0.49 - ETA: 24:55 - loss: 0.7144 - acc: 0.49 - ETA: 24:30 - loss: 0.7144 - acc: 0.49 - ETA: 24:06 - loss: 0.7144 - acc: 0.49 - ETA: 23:41 - loss: 0.7143 - acc: 0.49 - ETA: 23:16 - loss: 0.7143 - acc: 0.49 - ETA: 22:51 - loss: 0.7143 - acc: 0.49 - ETA: 22:26 - loss: 0.7143 - acc: 0.49 - ETA: 22:01 - loss: 0.7143 - acc: 0.49 - ETA: 21:36 - loss: 0.7143 - acc: 0.49 - ETA: 21:12 - loss: 0.7142 - acc: 0.49 - ETA: 20:47 - loss: 0.7142 - acc: 0.49 - ETA: 20:22 - loss: 0.7142 - acc: 0.49 - ETA: 19:57 - loss: 0.7142 - acc: 0.49 - ETA: 19:32 - loss: 0.7141 - acc: 0.49 - ETA: 19:07 - loss: 0.7141 - acc: 0.49 - ETA: 18:42 - loss: 0.7141 - acc: 0.49 - ETA: 18:18 - loss: 0.7140 - acc: 0.49 - ETA: 17:53 - loss: 0.7140 - acc: 0.49 - ETA: 17:28 - loss: 0.7140 - acc: 0.49 - ETA: 17:04 - loss: 0.7140 - acc: 0.49 - ETA: 16:39 - loss: 0.7139 - acc: 0.49 - ETA: 16:14 - loss: 0.7139 - acc: 0.49 - ETA: 15:49 - loss: 0.7138 - acc: 0.49 - ETA: 15:24 - loss: 0.7138 - acc: 0.49 - ETA: 14:59 - loss: 0.7139 - acc: 0.49 - ETA: 14:34 - loss: 0.7138 - acc: 0.49 - ETA: 14:10 - loss: 0.7139 - acc: 0.49 - ETA: 13:45 - loss: 0.7138 - acc: 0.49 - ETA: 13:20 - loss: 0.7138 - acc: 0.49 - ETA: 12:55 - loss: 0.7137 - acc: 0.49 - ETA: 12:30 - loss: 0.7137 - acc: 0.49 - ETA: 12:06 - loss: 0.7137 - acc: 0.49 - ETA: 11:41 - loss: 0.7137 - acc: 0.49 - ETA: 11:16 - loss: 0.7136 - acc: 0.49 - ETA: 10:51 - loss: 0.7136 - acc: 0.49 - ETA: 10:26 - loss: 0.7136 - acc: 0.49 - ETA: 10:02 - loss: 0.7136 - acc: 0.49 - ETA: 9:37 - loss: 0.7135 - acc: 0.4993 - ETA: 9:12 - loss: 0.7135 - acc: 0.499 - ETA: 8:47 - loss: 0.7135 - acc: 0.499 - ETA: 8:22 - loss: 0.7135 - acc: 0.499 - ETA: 7:57 - loss: 0.7135 - acc: 0.499 - ETA: 7:33 - loss: 0.7134 - acc: 0.499 - ETA: 7:08 - loss: 0.7134 - acc: 0.499 - ETA: 6:43 - loss: 0.7134 - acc: 0.499 - ETA: 6:18 - loss: 0.7134 - acc: 0.499 - ETA: 5:53 - loss: 0.7133 - acc: 0.499 - ETA: 5:28 - loss: 0.7133 - acc: 0.499 - ETA: 5:04 - loss: 0.7133 - acc: 0.499 - ETA: 4:39 - loss: 0.7133 - acc: 0.498 - ETA: 4:14 - loss: 0.7132 - acc: 0.499 - ETA: 3:49 - loss: 0.7132 - acc: 0.499 - ETA: 3:24 - loss: 0.7132 - acc: 0.499 - ETA: 2:59 - loss: 0.7132 - acc: 0.499 - ETA: 2:35 - loss: 0.7131 - acc: 0.499 - ETA: 2:10 - loss: 0.7131 - acc: 0.499 - ETA: 1:45 - loss: 0.7131 - acc: 0.499 - ETA: 1:20 - loss: 0.7130 - acc: 0.499 - ETA: 55s - loss: 0.7130 - acc: 0.499 - ETA: 31s - loss: 0.7130 - acc: 0.49 - ETA: 6s - loss: 0.7130 - acc: 0.4995 - 35716s 1s/sample - loss: 0.7130 - acc: 0.4995 - val_loss: 0.6959 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1990bb262e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_5 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          196864      bert_layer_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            257         dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 3,147,009\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_6 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          196864      bert_layer_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            257         dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 3,147,009\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('BertModel.h5')\n",
    "pre_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions before we clear and reload model\n",
    "\n",
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel.h5')\n",
    "\n",
    "post_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions after we clear and reload model\n",
    "all(pre_save_preds == post_save_preds) # Are they the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
